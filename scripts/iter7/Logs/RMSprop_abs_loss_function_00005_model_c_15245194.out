Loaded dependency [python3/3.10.7]: gcc/11.3.0-binutils-2.38
Loaded module: python3/3.10.7

Loading python3/3.10.7
  Loading requirement: gcc/11.3.0-binutils-2.38
Loaded module: cuda/11.6
Loaded module: cudnn/v8.3.2.44-prod-cuda-11.X
2023-01-27 15:20:32.909523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-27 15:20:37.251927: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /appl/cudnn/v8.3.2.44-prod-cuda-11.5/lib:/appl/cuda/11.6.0/lib64:/appl/python/3.10.7/lib:/appl/gcc/11.3.0-binutils-2.38/lib64:/appl/gcc/11.3.0-binutils-2.38/lib:/lsf/10.1/linux3.10-glibc2.17-x86_64/lib
2023-01-27 15:20:37.252645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /appl/cudnn/v8.3.2.44-prod-cuda-11.5/lib:/appl/cuda/11.6.0/lib64:/appl/python/3.10.7/lib:/appl/gcc/11.3.0-binutils-2.38/lib64:/appl/gcc/11.3.0-binutils-2.38/lib:/lsf/10.1/linux3.10-glibc2.17-x86_64/lib
2023-01-27 15:20:37.252657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-27 15:20:44.140061: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-27 15:20:46.272181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30961 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0
Epochs:  100
Optimizer:  RMSprop(learning_rate=0.0005)
Loss function name:  abs_loss_function
Csv file used:  black_background_500x500.csv
Verbose:  2
Unique name:  RMSprop_abs_loss_function_00005_model_c_
Output folder:  iter7
Model name:  load_model_c
 
        ################ MODEL ############### 
 
        inputs = keras.Input(shape=(input_size, input_size, 1))
        x = layers.Conv2D(filters=16, kernel_size=11, activation="relu")(inputs)
        x = layers.Conv2D(filters=16, kernel_size=7, activation="relu")(x)
        x = layers.MaxPooling2D(pool_size=2)(x)
        x = layers.Conv2D(filters=16, kernel_size=5, activation="relu")(x)
        x = layers.Conv2D(filters=16, kernel_size=3, activation="relu")(x)
        x = layers.MaxPooling2D(pool_size=2)(x)
        x = layers.Conv2D(filters=16, kernel_size=3, activation="relu")(x)
        x = layers.Conv2D(filters=16, kernel_size=3, activation="relu")(x)
        x = layers.MaxPooling2D(pool_size=2)(x)
        x = layers.Conv2D(filters=16, kernel_size=3, activation="relu")(x)
        x = layers.Conv2D(filters=16, kernel_size=3, activation="relu")(x)
        x = layers.MaxPooling2D(pool_size=2)(x)
        x = layers.Conv2D(filters=16, kernel_size=3, activation="relu")(x)
        x = layers.Conv2D(filters=16, kernel_size=3, activation="relu")(x)
        x = layers.MaxPooling2D(pool_size=2)(x)
        x = layers.Flatten()(x)
        x = layers.Dense(128, activation="relu", kernel_regularizer=keras.regularizers.l2(0.01))(x)
        x = layers.Dense(16, activation="relu", kernel_regularizer=keras.regularizers.l2(0.01))(x) 
        outputs = layers.Dense(3)(x)

        model = keras.Model(inputs=inputs, outputs=outputs)
    
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 500, 500, 1)]     0         
                                                                 
 conv2d (Conv2D)             (None, 490, 490, 16)      1952      
                                                                 
 conv2d_1 (Conv2D)           (None, 484, 484, 16)      12560     
                                                                 
 max_pooling2d (MaxPooling2D  (None, 242, 242, 16)     0         
 )                                                               
                                                                 
 conv2d_2 (Conv2D)           (None, 238, 238, 16)      6416      
                                                                 
 conv2d_3 (Conv2D)           (None, 236, 236, 16)      2320      
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 118, 118, 16)     0         
 2D)                                                             
                                                                 
 conv2d_4 (Conv2D)           (None, 116, 116, 16)      2320      
                                                                 
 conv2d_5 (Conv2D)           (None, 114, 114, 16)      2320      
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 57, 57, 16)       0         
 2D)                                                             
                                                                 
 conv2d_6 (Conv2D)           (None, 55, 55, 16)        2320      
                                                                 
 conv2d_7 (Conv2D)           (None, 53, 53, 16)        2320      
                                                                 
 max_pooling2d_3 (MaxPooling  (None, 26, 26, 16)       0         
 2D)                                                             
                                                                 
 conv2d_8 (Conv2D)           (None, 24, 24, 16)        2320      
                                                                 
 conv2d_9 (Conv2D)           (None, 22, 22, 16)        2320      
                                                                 
 max_pooling2d_4 (MaxPooling  (None, 11, 11, 16)       0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 1936)              0         
                                                                 
 dense (Dense)               (None, 128)               247936    
                                                                 
 dense_1 (Dense)             (None, 16)                2064      
                                                                 
 dense_2 (Dense)             (None, 3)                 51        
                                                                 
=================================================================
Total params: 287,219
Trainable params: 287,219
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
2023-01-27 15:20:50.659413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8302
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 1/31 [..............................] - ETA: 8s - loss: 0.2740 - abs_loss_function: 0.2599 - accuracy: 0.4062 2/31 [>.............................] - ETA: 6s - loss: 0.2726 - abs_loss_function: 0.2585 - accuracy: 0.4062 3/31 [=>............................] - ETA: 6s - loss: 0.2747 - abs_loss_function: 0.2606 - accuracy: 0.4167 4/31 [==>...........................] - ETA: 7s - loss: 0.2742 - abs_loss_function: 0.2601 - accuracy: 0.4219 5/31 [===>..........................] - ETA: 6s - loss: 0.2740 - abs_loss_function: 0.2599 - accuracy: 0.4250 6/31 [====>.........................] - ETA: 6s - loss: 0.2731 - abs_loss_function: 0.2590 - accuracy: 0.4271 7/31 [=====>........................] - ETA: 6s - loss: 0.2720 - abs_loss_function: 0.2579 - accuracy: 0.4286 8/31 [======>.......................] - ETA: 5s - loss: 0.2708 - abs_loss_function: 0.2568 - accuracy: 0.4297 9/31 [=======>......................] - ETA: 5s - loss: 0.2704 - abs_loss_function: 0.2563 - accuracy: 0.427110/31 [========>.....................] - ETA: 5s - loss: 0.2695 - abs_loss_function: 0.2554 - accuracy: 0.425011/31 [=========>....................] - ETA: 4s - loss: 0.2691 - abs_loss_function: 0.2550 - accuracy: 0.423312/31 [==========>...................] - ETA: 4s - loss: 0.2689 - abs_loss_function: 0.2548 - accuracy: 0.421913/31 [===========>..................] - ETA: 4s - loss: 0.2688 - abs_loss_function: 0.2548 - accuracy: 0.420714/31 [============>.................] - ETA: 4s - loss: 0.2689 - abs_loss_function: 0.2548 - accuracy: 0.419615/31 [=============>................] - ETA: 3s - loss: 0.2690 - abs_loss_function: 0.2549 - accuracy: 0.418716/31 [==============>...............] - ETA: 3s - loss: 0.2686 - abs_loss_function: 0.2545 - accuracy: 0.416017/31 [===============>..............] - ETA: 3s - loss: 0.2684 - abs_loss_function: 0.2543 - accuracy: 0.415418/31 [================>.............] - ETA: 3s - loss: 0.2682 - abs_loss_function: 0.2542 - accuracy: 0.416719/31 [=================>............] - ETA: 3s - loss: 0.2679 - abs_loss_function: 0.2539 - accuracy: 0.416120/31 [==================>...........] - ETA: 2s - loss: 0.2678 - abs_loss_function: 0.2537 - accuracy: 0.415621/31 [===================>..........] - ETA: 2s - loss: 0.2674 - abs_loss_function: 0.2534 - accuracy: 0.415222/31 [====================>.........] - ETA: 2s - loss: 0.2672 - abs_loss_function: 0.2531 - accuracy: 0.414823/31 [=====================>........] - ETA: 1s - loss: 0.2671 - abs_loss_function: 0.2530 - accuracy: 0.413024/31 [======================>.......] - ETA: 1s - loss: 0.2669 - abs_loss_function: 0.2528 - accuracy: 0.412825/31 [=======================>......] - ETA: 1s - loss: 0.2666 - abs_loss_function: 0.2525 - accuracy: 0.411226/31 [========================>.....] - ETA: 1s - loss: 0.2663 - abs_loss_function: 0.2523 - accuracy: 0.409927/31 [=========================>....] - ETA: 1s - loss: 0.2661 - abs_loss_function: 0.2521 - accuracy: 0.408628/31 [==========================>...] - ETA: 0s - loss: 0.2658 - abs_loss_function: 0.2517 - accuracy: 0.407429/31 [===========================>..] - ETA: 0s - loss: 0.2655 - abs_loss_function: 0.2514 - accuracy: 0.405230/31 [============================>.] - ETA: 0s - loss: 0.2650 - abs_loss_function: 0.2510 - accuracy: 0.403131/31 [==============================] - ETA: 0s - loss: 0.2646 - abs_loss_function: 0.2506 - accuracy: 0.401231/31 [==============================] - 8s 247ms/step - loss: 0.2646 - abs_loss_function: 0.2506 - accuracy: 0.4012
/zhome/ab/7/153983/project/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:409: UserWarning: grayscale is deprecated. Please use color_mode = "grayscale"
  warnings.warn(
Loss on test data:  0.26461702585220337
----------0----------
phi1 54.7
PHI 36.1
phi2 23.5
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 266ms/step
predicted values [[-4.263993  -2.3724613  3.888755 ]]
----------1----------
phi1 76.0
PHI 83.7
phi2 2.9
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 19ms/step
predicted values [[-4.2406397 -2.3759246  3.8744013]]
----------2----------
phi1 17.8
PHI 63.8
phi2 50.8
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 22ms/step
predicted values [[-4.265544  -2.3729029  3.8894546]]
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
273/273 - 93s - loss: 0.5605 - abs_loss_function: 0.2418 - accuracy: 0.3156 - val_loss: 0.2755 - val_abs_loss_function: 0.2615 - val_accuracy: 0.2923 - 93s/epoch - 339ms/step
Epoch 2/100
273/273 - 75s - loss: 0.2446 - abs_loss_function: 0.2415 - accuracy: 0.3077 - val_loss: 0.2625 - val_abs_loss_function: 0.2616 - val_accuracy: 0.3558 - 75s/epoch - 276ms/step
Epoch 3/100
273/273 - 79s - loss: 0.2559 - abs_loss_function: 0.2541 - accuracy: 0.3351 - val_loss: 0.2625 - val_abs_loss_function: 0.2609 - val_accuracy: 0.4214 - 79s/epoch - 288ms/step
Epoch 4/100
273/273 - 71s - loss: 0.2585 - abs_loss_function: 0.2571 - accuracy: 0.3215 - val_loss: 0.2503 - val_abs_loss_function: 0.2487 - val_accuracy: 0.2954 - 71s/epoch - 261ms/step
Epoch 5/100
273/273 - 67s - loss: 0.2475 - abs_loss_function: 0.2463 - accuracy: 0.3333 - val_loss: 0.2491 - val_abs_loss_function: 0.2476 - val_accuracy: 0.2389 - 67s/epoch - 244ms/step
Epoch 6/100
273/273 - 65s - loss: 0.2434 - abs_loss_function: 0.2418 - accuracy: 0.3304 - val_loss: 0.2654 - val_abs_loss_function: 0.2646 - val_accuracy: 0.2550 - 65s/epoch - 239ms/step
Epoch 7/100
273/273 - 67s - loss: 0.2434 - abs_loss_function: 0.2419 - accuracy: 0.3174 - val_loss: 0.2375 - val_abs_loss_function: 0.2362 - val_accuracy: 0.3337 - 67s/epoch - 244ms/step
Epoch 8/100
273/273 - 66s - loss: 0.2441 - abs_loss_function: 0.2430 - accuracy: 0.3040 - val_loss: 0.2537 - val_abs_loss_function: 0.2526 - val_accuracy: 0.3831 - 66s/epoch - 241ms/step
Epoch 9/100
273/273 - 65s - loss: 0.2468 - abs_loss_function: 0.2456 - accuracy: 0.3109 - val_loss: 0.2586 - val_abs_loss_function: 0.2579 - val_accuracy: 0.3609 - 65s/epoch - 240ms/step
Epoch 10/100
273/273 - 67s - loss: 0.2453 - abs_loss_function: 0.2446 - accuracy: 0.2845 - val_loss: 0.2469 - val_abs_loss_function: 0.2449 - val_accuracy: 0.3125 - 67s/epoch - 246ms/step
Epoch 11/100
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 1/31 [..............................] - ETA: 8s - loss: 0.2412 - abs_loss_function: 0.2407 - accuracy: 0.3750 2/31 [>.............................] - ETA: 6s - loss: 0.2381 - abs_loss_function: 0.2375 - accuracy: 0.3594 3/31 [=>............................] - ETA: 5s - loss: 0.2382 - abs_loss_function: 0.2376 - accuracy: 0.3646 4/31 [==>...........................] - ETA: 5s - loss: 0.2375 - abs_loss_function: 0.2370 - accuracy: 0.3750 5/31 [===>..........................] - ETA: 5s - loss: 0.2366 - abs_loss_function: 0.2361 - accuracy: 0.3750 6/31 [====>.........................] - ETA: 5s - loss: 0.2362 - abs_loss_function: 0.2357 - accuracy: 0.3698 7/31 [=====>........................] - ETA: 4s - loss: 0.2358 - abs_loss_function: 0.2352 - accuracy: 0.3616 8/31 [======>.......................] - ETA: 4s - loss: 0.2355 - abs_loss_function: 0.2349 - accuracy: 0.3555 9/31 [=======>......................] - ETA: 4s - loss: 0.2361 - abs_loss_function: 0.2355 - accuracy: 0.350710/31 [========>.....................] - ETA: 4s - loss: 0.2365 - abs_loss_function: 0.2359 - accuracy: 0.346911/31 [=========>....................] - ETA: 4s - loss: 0.2376 - abs_loss_function: 0.2371 - accuracy: 0.346612/31 [==========>...................] - ETA: 3s - loss: 0.2384 - abs_loss_function: 0.2378 - accuracy: 0.343813/31 [===========>..................] - ETA: 3s - loss: 0.2391 - abs_loss_function: 0.2386 - accuracy: 0.341314/31 [============>.................] - ETA: 3s - loss: 0.2395 - abs_loss_function: 0.2389 - accuracy: 0.339315/31 [=============>................] - ETA: 3s - loss: 0.2399 - abs_loss_function: 0.2393 - accuracy: 0.337516/31 [==============>...............] - ETA: 3s - loss: 0.2406 - abs_loss_function: 0.2400 - accuracy: 0.335917/31 [===============>..............] - ETA: 2s - loss: 0.2410 - abs_loss_function: 0.2404 - accuracy: 0.334618/31 [================>.............] - ETA: 2s - loss: 0.2415 - abs_loss_function: 0.2409 - accuracy: 0.333319/31 [=================>............] - ETA: 2s - loss: 0.2421 - abs_loss_function: 0.2416 - accuracy: 0.333920/31 [==================>...........] - ETA: 2s - loss: 0.2426 - abs_loss_function: 0.2420 - accuracy: 0.334421/31 [===================>..........] - ETA: 2s - loss: 0.2425 - abs_loss_function: 0.2419 - accuracy: 0.333322/31 [====================>.........] - ETA: 1s - loss: 0.2424 - abs_loss_function: 0.2419 - accuracy: 0.332423/31 [=====================>........] - ETA: 1s - loss: 0.2425 - abs_loss_function: 0.2420 - accuracy: 0.331524/31 [======================>.......] - ETA: 1s - loss: 0.2426 - abs_loss_function: 0.2421 - accuracy: 0.330725/31 [=======================>......] - ETA: 1s - loss: 0.2428 - abs_loss_function: 0.2423 - accuracy: 0.331326/31 [========================>.....] - ETA: 1s - loss: 0.2431 - abs_loss_function: 0.2425 - accuracy: 0.331727/31 [=========================>....] - ETA: 0s - loss: 0.2436 - abs_loss_function: 0.2430 - accuracy: 0.333328/31 [==========================>...] - ETA: 0s - loss: 0.2441 - abs_loss_function: 0.2435 - accuracy: 0.334829/31 [===========================>..] - ETA: 0s - loss: 0.2445 - abs_loss_function: 0.2439 - accuracy: 0.335130/31 [============================>.] - ETA: 0s - loss: 0.2447 - abs_loss_function: 0.2441 - accuracy: 0.335431/31 [==============================] - ETA: 0s - loss: 0.2446 - abs_loss_function: 0.2441 - accuracy: 0.335731/31 [==============================] - 6s 207ms/step - loss: 0.2446 - abs_loss_function: 0.2441 - accuracy: 0.3357
Loss on test data:  0.2446262538433075
----------0----------
phi1 54.7
PHI 36.1
phi2 23.5
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 19ms/step
predicted values [[-17.102156   -3.4397125  14.173929 ]]
----------1----------
phi1 76.0
PHI 83.7
phi2 2.9
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 18ms/step
predicted values [[-17.102135   -3.4397194  14.173907 ]]
----------2----------
phi1 17.8
PHI 63.8
phi2 50.8
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 18ms/step
predicted values [[-17.102175   -3.4397068  14.173947 ]]
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
273/273 - 72s - loss: 0.2426 - abs_loss_function: 0.2417 - accuracy: 0.2899 - val_loss: 0.2513 - val_abs_loss_function: 0.2507 - val_accuracy: 0.3206 - 72s/epoch - 265ms/step
Epoch 12/100
273/273 - 63s - loss: 0.2459 - abs_loss_function: 0.2450 - accuracy: 0.3349 - val_loss: 0.2573 - val_abs_loss_function: 0.2565 - val_accuracy: 0.2228 - 63s/epoch - 231ms/step
Epoch 13/100
273/273 - 63s - loss: 0.2448 - abs_loss_function: 0.2437 - accuracy: 0.3120 - val_loss: 0.2403 - val_abs_loss_function: 0.2394 - val_accuracy: 0.3468 - 63s/epoch - 230ms/step
Epoch 14/100
273/273 - 63s - loss: 0.2369 - abs_loss_function: 0.2365 - accuracy: 0.3098 - val_loss: 0.2644 - val_abs_loss_function: 0.2639 - val_accuracy: 0.4536 - 63s/epoch - 230ms/step
Epoch 15/100
273/273 - 63s - loss: 0.2390 - abs_loss_function: 0.2385 - accuracy: 0.3115 - val_loss: 0.2518 - val_abs_loss_function: 0.2512 - val_accuracy: 0.4425 - 63s/epoch - 231ms/step
Epoch 16/100
273/273 - 65s - loss: 0.2437 - abs_loss_function: 0.2430 - accuracy: 0.3455 - val_loss: 0.2722 - val_abs_loss_function: 0.2719 - val_accuracy: 0.3407 - 65s/epoch - 240ms/step
Epoch 17/100
273/273 - 66s - loss: 0.2445 - abs_loss_function: 0.2435 - accuracy: 0.3262 - val_loss: 0.2538 - val_abs_loss_function: 0.2530 - val_accuracy: 0.3518 - 66s/epoch - 241ms/step
Epoch 18/100
273/273 - 65s - loss: 0.2447 - abs_loss_function: 0.2429 - accuracy: 0.3725 - val_loss: 0.2591 - val_abs_loss_function: 0.2568 - val_accuracy: 0.2742 - 65s/epoch - 237ms/step
Epoch 19/100
273/273 - 65s - loss: 0.2435 - abs_loss_function: 0.2419 - accuracy: 0.3347 - val_loss: 0.2558 - val_abs_loss_function: 0.2544 - val_accuracy: 0.3075 - 65s/epoch - 239ms/step
Epoch 20/100
273/273 - 72s - loss: 0.2387 - abs_loss_function: 0.2378 - accuracy: 0.2866 - val_loss: 0.2517 - val_abs_loss_function: 0.2513 - val_accuracy: 0.3579 - 72s/epoch - 263ms/step
Epoch 21/100
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 1/31 [..............................] - ETA: 13s - loss: 0.2414 - abs_loss_function: 0.2412 - accuracy: 0.3438 2/31 [>.............................] - ETA: 11s - loss: 0.2428 - abs_loss_function: 0.2425 - accuracy: 0.3438 3/31 [=>............................] - ETA: 12s - loss: 0.2437 - abs_loss_function: 0.2435 - accuracy: 0.3542 4/31 [==>...........................] - ETA: 11s - loss: 0.2425 - abs_loss_function: 0.2423 - accuracy: 0.3594 5/31 [===>..........................] - ETA: 11s - loss: 0.2413 - abs_loss_function: 0.2410 - accuracy: 0.3625 6/31 [====>.........................] - ETA: 11s - loss: 0.2399 - abs_loss_function: 0.2396 - accuracy: 0.3698 7/31 [=====>........................] - ETA: 10s - loss: 0.2387 - abs_loss_function: 0.2384 - accuracy: 0.3795 8/31 [======>.......................] - ETA: 10s - loss: 0.2379 - abs_loss_function: 0.2377 - accuracy: 0.3867 9/31 [=======>......................] - ETA: 9s - loss: 0.2374 - abs_loss_function: 0.2371 - accuracy: 0.3924 10/31 [========>.....................] - ETA: 9s - loss: 0.2368 - abs_loss_function: 0.2366 - accuracy: 0.396911/31 [=========>....................] - ETA: 8s - loss: 0.2357 - abs_loss_function: 0.2354 - accuracy: 0.400612/31 [==========>...................] - ETA: 8s - loss: 0.2349 - abs_loss_function: 0.2346 - accuracy: 0.403613/31 [===========>..................] - ETA: 7s - loss: 0.2339 - abs_loss_function: 0.2336 - accuracy: 0.406214/31 [============>.................] - ETA: 7s - loss: 0.2332 - abs_loss_function: 0.2330 - accuracy: 0.410715/31 [=============>................] - ETA: 6s - loss: 0.2330 - abs_loss_function: 0.2327 - accuracy: 0.412516/31 [==============>...............] - ETA: 6s - loss: 0.2329 - abs_loss_function: 0.2326 - accuracy: 0.414117/31 [===============>..............] - ETA: 5s - loss: 0.2329 - abs_loss_function: 0.2326 - accuracy: 0.415418/31 [================>.............] - ETA: 5s - loss: 0.2331 - abs_loss_function: 0.2328 - accuracy: 0.418419/31 [=================>............] - ETA: 5s - loss: 0.2333 - abs_loss_function: 0.2330 - accuracy: 0.419420/31 [==================>...........] - ETA: 4s - loss: 0.2337 - abs_loss_function: 0.2334 - accuracy: 0.420321/31 [===================>..........] - ETA: 4s - loss: 0.2339 - abs_loss_function: 0.2336 - accuracy: 0.421122/31 [====================>.........] - ETA: 3s - loss: 0.2343 - abs_loss_function: 0.2340 - accuracy: 0.421923/31 [=====================>........] - ETA: 3s - loss: 0.2343 - abs_loss_function: 0.2341 - accuracy: 0.422624/31 [======================>.......] - ETA: 3s - loss: 0.2345 - abs_loss_function: 0.2343 - accuracy: 0.423225/31 [=======================>......] - ETA: 2s - loss: 0.2348 - abs_loss_function: 0.2345 - accuracy: 0.425026/31 [========================>.....] - ETA: 2s - loss: 0.2350 - abs_loss_function: 0.2347 - accuracy: 0.426727/31 [=========================>....] - ETA: 1s - loss: 0.2352 - abs_loss_function: 0.2349 - accuracy: 0.429428/31 [==========================>...] - ETA: 1s - loss: 0.2353 - abs_loss_function: 0.2350 - accuracy: 0.431929/31 [===========================>..] - ETA: 0s - loss: 0.2353 - abs_loss_function: 0.2350 - accuracy: 0.433230/31 [============================>.] - ETA: 0s - loss: 0.2353 - abs_loss_function: 0.2350 - accuracy: 0.434431/31 [==============================] - ETA: 0s - loss: 0.2355 - abs_loss_function: 0.2352 - accuracy: 0.436531/31 [==============================] - 14s 451ms/step - loss: 0.2355 - abs_loss_function: 0.2352 - accuracy: 0.4365
Loss on test data:  0.23545022308826447
----------0----------
phi1 54.7
PHI 36.1
phi2 23.5
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 20ms/step
predicted values [[-10.410629   -4.0706983  10.156333 ]]
----------1----------
phi1 76.0
PHI 83.7
phi2 2.9
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 19ms/step
predicted values [[-10.410629   -4.0706983  10.156333 ]]
----------2----------
phi1 17.8
PHI 63.8
phi2 50.8
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 19ms/step
predicted values [[-10.410629   -4.0706983  10.156333 ]]
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
273/273 - 106s - loss: 0.2403 - abs_loss_function: 0.2397 - accuracy: 0.2881 - val_loss: 0.2441 - val_abs_loss_function: 0.2438 - val_accuracy: 0.3569 - 106s/epoch - 388ms/step
Epoch 22/100
273/273 - 148s - loss: 0.2576 - abs_loss_function: 0.2572 - accuracy: 0.3555 - val_loss: 0.2483 - val_abs_loss_function: 0.2479 - val_accuracy: 0.4123 - 148s/epoch - 542ms/step
Epoch 23/100
273/273 - 155s - loss: 0.2441 - abs_loss_function: 0.2435 - accuracy: 0.2558 - val_loss: 0.2677 - val_abs_loss_function: 0.2669 - val_accuracy: 0.4042 - 155s/epoch - 566ms/step
Epoch 24/100
273/273 - 154s - loss: 0.2486 - abs_loss_function: 0.2481 - accuracy: 0.3346 - val_loss: 0.2628 - val_abs_loss_function: 0.2619 - val_accuracy: 0.2319 - 154s/epoch - 564ms/step
Epoch 25/100
273/273 - 156s - loss: 0.2473 - abs_loss_function: 0.2465 - accuracy: 0.2905 - val_loss: 0.2382 - val_abs_loss_function: 0.2372 - val_accuracy: 0.2137 - 156s/epoch - 572ms/step
Epoch 26/100
273/273 - 159s - loss: 0.2451 - abs_loss_function: 0.2437 - accuracy: 0.3352 - val_loss: 0.2235 - val_abs_loss_function: 0.2223 - val_accuracy: 0.3448 - 159s/epoch - 581ms/step
Epoch 27/100
273/273 - 156s - loss: 0.2493 - abs_loss_function: 0.2485 - accuracy: 0.3330 - val_loss: 0.2528 - val_abs_loss_function: 0.2521 - val_accuracy: 0.2853 - 156s/epoch - 573ms/step
Epoch 28/100
273/273 - 147s - loss: 0.2454 - abs_loss_function: 0.2443 - accuracy: 0.3381 - val_loss: 0.2697 - val_abs_loss_function: 0.2687 - val_accuracy: 0.3337 - 147s/epoch - 540ms/step
Epoch 29/100
273/273 - 154s - loss: 0.2524 - abs_loss_function: 0.2518 - accuracy: 0.3284 - val_loss: 0.2582 - val_abs_loss_function: 0.2577 - val_accuracy: 0.4194 - 154s/epoch - 564ms/step
Epoch 30/100
273/273 - 157s - loss: 0.2447 - abs_loss_function: 0.2437 - accuracy: 0.3168 - val_loss: 0.2748 - val_abs_loss_function: 0.2736 - val_accuracy: 0.3679 - 157s/epoch - 574ms/step
Epoch 31/100
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 1/31 [..............................] - ETA: 16s - loss: 0.2551 - abs_loss_function: 0.2537 - accuracy: 0.3438 2/31 [>.............................] - ETA: 14s - loss: 0.2565 - abs_loss_function: 0.2551 - accuracy: 0.3438 3/31 [=>............................] - ETA: 12s - loss: 0.2567 - abs_loss_function: 0.2553 - accuracy: 0.3438 4/31 [==>...........................] - ETA: 12s - loss: 0.2559 - abs_loss_function: 0.2545 - accuracy: 0.3438 5/31 [===>..........................] - ETA: 12s - loss: 0.2547 - abs_loss_function: 0.2533 - accuracy: 0.3500 6/31 [====>.........................] - ETA: 11s - loss: 0.2541 - abs_loss_function: 0.2528 - accuracy: 0.3490 7/31 [=====>........................] - ETA: 11s - loss: 0.2534 - abs_loss_function: 0.2520 - accuracy: 0.3438 8/31 [======>.......................] - ETA: 10s - loss: 0.2528 - abs_loss_function: 0.2514 - accuracy: 0.3398 9/31 [=======>......................] - ETA: 10s - loss: 0.2518 - abs_loss_function: 0.2504 - accuracy: 0.333310/31 [========>.....................] - ETA: 10s - loss: 0.2508 - abs_loss_function: 0.2494 - accuracy: 0.328111/31 [=========>....................] - ETA: 9s - loss: 0.2498 - abs_loss_function: 0.2485 - accuracy: 0.3267 12/31 [==========>...................] - ETA: 8s - loss: 0.2494 - abs_loss_function: 0.2480 - accuracy: 0.325513/31 [===========>..................] - ETA: 8s - loss: 0.2488 - abs_loss_function: 0.2474 - accuracy: 0.324514/31 [============>.................] - ETA: 7s - loss: 0.2480 - abs_loss_function: 0.2466 - accuracy: 0.325915/31 [=============>................] - ETA: 7s - loss: 0.2473 - abs_loss_function: 0.2459 - accuracy: 0.327116/31 [==============>...............] - ETA: 6s - loss: 0.2469 - abs_loss_function: 0.2455 - accuracy: 0.326217/31 [===============>..............] - ETA: 6s - loss: 0.2468 - abs_loss_function: 0.2454 - accuracy: 0.323518/31 [================>.............] - ETA: 5s - loss: 0.2466 - abs_loss_function: 0.2452 - accuracy: 0.321219/31 [=================>............] - ETA: 5s - loss: 0.2463 - abs_loss_function: 0.2449 - accuracy: 0.319120/31 [==================>...........] - ETA: 5s - loss: 0.2463 - abs_loss_function: 0.2449 - accuracy: 0.317221/31 [===================>..........] - ETA: 4s - loss: 0.2462 - abs_loss_function: 0.2448 - accuracy: 0.315522/31 [====================>.........] - ETA: 4s - loss: 0.2461 - abs_loss_function: 0.2447 - accuracy: 0.313923/31 [=====================>........] - ETA: 3s - loss: 0.2460 - abs_loss_function: 0.2447 - accuracy: 0.313924/31 [======================>.......] - ETA: 3s - loss: 0.2462 - abs_loss_function: 0.2448 - accuracy: 0.313825/31 [=======================>......] - ETA: 2s - loss: 0.2463 - abs_loss_function: 0.2449 - accuracy: 0.313726/31 [========================>.....] - ETA: 2s - loss: 0.2464 - abs_loss_function: 0.2450 - accuracy: 0.313727/31 [=========================>....] - ETA: 1s - loss: 0.2466 - abs_loss_function: 0.2452 - accuracy: 0.314828/31 [==========================>...] - ETA: 1s - loss: 0.2466 - abs_loss_function: 0.2452 - accuracy: 0.315829/31 [===========================>..] - ETA: 0s - loss: 0.2467 - abs_loss_function: 0.2453 - accuracy: 0.315730/31 [============================>.] - ETA: 0s - loss: 0.2467 - abs_loss_function: 0.2453 - accuracy: 0.315631/31 [==============================] - ETA: 0s - loss: 0.2467 - abs_loss_function: 0.2453 - accuracy: 0.315531/31 [==============================] - 14s 465ms/step - loss: 0.2467 - abs_loss_function: 0.2453 - accuracy: 0.3155
Loss on test data:  0.24666620790958405
----------0----------
phi1 54.7
PHI 36.1
phi2 23.5
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 27ms/step
predicted values [[-54.242832  71.64428   57.279266]]
----------1----------
phi1 76.0
PHI 83.7
phi2 2.9
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 19ms/step
predicted values [[-54.242832  71.64428   57.279266]]
----------2----------
phi1 17.8
PHI 63.8
phi2 50.8
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 19ms/step
predicted values [[-54.242832  71.64428   57.279266]]
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
273/273 - 165s - loss: 0.2478 - abs_loss_function: 0.2469 - accuracy: 0.3733 - val_loss: 0.2723 - val_abs_loss_function: 0.2709 - val_accuracy: 0.3679 - 165s/epoch - 606ms/step
Epoch 32/100
Terminated

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 15245194: <s202741-train> in cluster <dcc> Exited

Job <s202741-train> was submitted from host <n-62-20-1> by user <s202741> in cluster <dcc> at Thu Jan 26 05:31:21 2023
Job was executed on host(s) <4*n-62-20-10>, in queue <gpuv100>, as user <s202741> in cluster <dcc> at Fri Jan 27 15:20:26 2023
</zhome/ab/7/153983> was used as the home directory.
</zhome/ab/7/153983/project/scripts> was used as the working directory.
Started at Fri Jan 27 15:20:26 2023
Terminated at Fri Jan 27 16:12:36 2023
Results reported at Fri Jan 27 16:12:36 2023

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input

#!/bin/bash
### General options
### -- specify queue --   NOTE: TitanX is significantly faster than K80
#BSUB -q gpuv100
#BSUB -gpu "num=1:mode=exclusive_process"
### -- set the job Name --
#BSUB -J s202741-train
### -- ask for number of cores (default: 1) --
#BSUB -n 4
#BSUB -R "span[hosts=1]"
### -- set walltime limit: hh:mm --  maximum 24 hours for GPU-queues right now
#BSUB -W 5:00
# request 5GB of memory
#BSUB -R "rusage[mem=5GB]"
### -- Specify the output and error file. %J is the job-id --
### -- -o and -e mean append, -oo and -eo mean overwrite --
#BSUB -o iter7/Logs/RMSprop_abs_loss_function_00005_model_c_%J.out
# -- end of LSF options --

# Necessary modules
cd ..
source venv/bin/activate

python trainModelIter4.py 100 "RMSprop(learning_rate=0.0005)" "abs_loss_function" "black_background_500x500.csv" 2 "RMSprop_abs_loss_function_00005_model_c_" "iter7" "load_model_c"

    
------------------------------------------------------------

Exited with exit code 143.

Resource usage summary:

    CPU time :                                   2812.00 sec.
    Max Memory :                                 3059 MB
    Average Memory :                             2617.67 MB
    Total Requested Memory :                     20480.00 MB
    Delta Memory :                               17421.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              4
    Max Threads :                                34
    Run time :                                   3131 sec.
    Turnaround time :                            124875 sec.

The output (if any) is above this job summary.

